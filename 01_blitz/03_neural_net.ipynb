{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a neural network based on the structure bellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"structure.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Não entendi o porque do subsampling (dividir as dimensões por 2)\n",
    "# entendi, tem o maxpooling\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # kernel\n",
    "        # 1 input channel, 6 output channels, 5x5 square convolution\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # 6 input channels, 16 output channels, 5x5 square convolutional\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        # affine operation: y = Wx + b\n",
    "        # flatten the output of conv2d -> 16 channels * 5 * 5 (image dimensions)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Max pooling over a 2,2 window\n",
    "        # The output of the first convolutional layer\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "\n",
    "        # The output of the second conv layer (2 or (2,2) will result on a square output)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        # Flatten the except the batch dimension\n",
    "        x = torch.flatten(x, start_dim =1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1624,  0.1251, -0.0329, -0.0932, -0.0174],\n",
      "          [-0.1920,  0.1147, -0.1692, -0.1977, -0.1986],\n",
      "          [ 0.1099, -0.1647,  0.1586, -0.1915, -0.0501],\n",
      "          [ 0.1504,  0.0763, -0.0617, -0.0063, -0.0932],\n",
      "          [ 0.0053,  0.1353,  0.0286,  0.1243,  0.1720]]],\n",
      "\n",
      "\n",
      "        [[[-0.1516, -0.1937, -0.1318, -0.0321, -0.1498],\n",
      "          [ 0.1985,  0.0065,  0.0699, -0.0989,  0.0735],\n",
      "          [ 0.1156, -0.1568, -0.0795,  0.1702,  0.0161],\n",
      "          [-0.1234, -0.1510, -0.1610, -0.0693, -0.0964],\n",
      "          [-0.0239,  0.1395, -0.0928, -0.0582, -0.0782]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0918,  0.0759,  0.1723, -0.0728, -0.1422],\n",
      "          [-0.1158,  0.0595, -0.0261, -0.0143, -0.1201],\n",
      "          [-0.0632,  0.1378,  0.1501, -0.1614, -0.0960],\n",
      "          [-0.0355, -0.1593,  0.1448, -0.0095,  0.1857],\n",
      "          [ 0.1499,  0.0027, -0.1408, -0.1105, -0.0541]]],\n",
      "\n",
      "\n",
      "        [[[-0.1835,  0.0477,  0.0063,  0.1634, -0.1938],\n",
      "          [ 0.0899, -0.1685,  0.0268,  0.0790,  0.1376],\n",
      "          [-0.1613,  0.1471, -0.0158,  0.0771, -0.1479],\n",
      "          [ 0.0323, -0.1601,  0.0585,  0.1383,  0.0235],\n",
      "          [-0.0921,  0.0079, -0.1707, -0.1136,  0.0594]]],\n",
      "\n",
      "\n",
      "        [[[-0.1773,  0.0020, -0.1989, -0.1768, -0.1564],\n",
      "          [ 0.1138, -0.0101, -0.1812,  0.1353,  0.0593],\n",
      "          [ 0.1734,  0.1663, -0.0405,  0.1080, -0.0112],\n",
      "          [-0.1416,  0.0287, -0.1222, -0.1368, -0.1050],\n",
      "          [-0.1610,  0.0028, -0.1140,  0.1440, -0.1639]]],\n",
      "\n",
      "\n",
      "        [[[-0.0492,  0.0552,  0.0023,  0.0438,  0.1832],\n",
      "          [ 0.0267, -0.0276,  0.0147, -0.1557,  0.0393],\n",
      "          [ 0.1247,  0.0062, -0.0947, -0.1842,  0.1844],\n",
      "          [-0.0643,  0.0613,  0.0085,  0.1311,  0.0796],\n",
      "          [-0.1366,  0.1417, -0.1652, -0.0352, -0.1306]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Manipulating the params\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0300,  0.0107, -0.0071,  0.0997,  0.0558, -0.0009, -0.0400,  0.1366,\n",
      "          0.1317, -0.1028]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation on the net\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the gradients into 0 and backpropagating random values\n",
    "net.zero_grad()\n",
    "out.backward(torch.rand(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([1, 10])\n",
      "target shape torch.Size([10])\n",
      "target shape torch.Size([1, 10])\n",
      "tensor(0.9187, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## LOSS FUNCTION\n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10) # dummy target\n",
    "\n",
    "print(f\"output shape {output.shape}\")\n",
    "print(f\"target shape {target.shape}\")\n",
    "\n",
    "# Reshaping the target\n",
    "target = target.view(1, -1)\n",
    "print(f\"target shape {target.shape}\")\n",
    "\n",
    "# Loss criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f41309fac10>\n",
      "<AddmmBackward object at 0x7f41309f6e20>\n",
      "<AccumulateGrad object at 0x7f4130e9de50>\n"
     ]
    }
   ],
   "source": [
    "# see the backward functions from loss to the beggining of the net\n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0107,  0.0041, -0.0026, -0.0037, -0.0013, -0.0055])\n"
     ]
    }
   ],
   "source": [
    "# Backpropagating the loss\n",
    "\n",
    "# Cleaning the gradients\n",
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "# applying the backward \n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UPDATING THE WEIGHTS\n",
    "\n",
    "## weight = weight - learning_rate * gradient\n",
    "\n",
    "# example of implementation\n",
    "# learning_rate = 0.01\n",
    "# for f in net.parameters():\n",
    "#     f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "\n",
    "# Using the built in package torch.optim\n",
    "import torch.optim as optim\n",
    "\n",
    "# creating the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "# in the training loop\n",
    "optimizer.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step() # updates the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cde81edf15c274bb0721111e8fc32fb5a80fe701fa2da5fabb1e915411a95d5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
